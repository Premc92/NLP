{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data using read_csv function from pandas package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e617e2489abe9bca</td>\n",
       "      <td>\"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9250cf637294e09d</td>\n",
       "      <td>\"\\r\\n\\r\\nThis seems unbalanced.  whatever I ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce1aa4592d5240ca</td>\n",
       "      <td>Marya Dzmitruk was born in Minsk, Belarus in M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48105766ff7f075b</td>\n",
       "      <td>\"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia...  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0543d4f82e5470b6</td>\n",
       "      <td>New Categories \\r\\n\\r\\nI honestly think that w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  e617e2489abe9bca  \"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The De...      0\n",
       "1  9250cf637294e09d  \"\\r\\n\\r\\nThis seems unbalanced.  whatever I ha...      0\n",
       "2  ce1aa4592d5240ca  Marya Dzmitruk was born in Minsk, Belarus in M...      0\n",
       "3  48105766ff7f075b      \"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia...  \"      0\n",
       "4  0543d4f82e5470b6  New Categories \\r\\n\\r\\nI honestly think that w...      0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "DESCRIPTION\n",
    "\n",
    "Using NLP and machine learning, make a model to identify toxic comments from the Talk edit pages on Wikipedia. Help identify the words that make a comment toxic.\n",
    "\n",
    "Problem Statement:  \n",
    "\n",
    "Wikipedia is the world’s largest and most popular reference work on the internet with about 500 million unique visitors per month. It also has millions of contributors who can make edits to pages. The Talk edit pages, the key community interaction forum where the contributing community interacts or discusses or debates about the changes pertaining to a particular topic. \n",
    "\n",
    "Wikipedia continuously strives to help online discussion become more productive and respectful. You are a data scientist at Wikipedia who will help Wikipedia to build a predictive model that identifies toxic comments in the discussion and marks them for cleanup by using NLP and machine learning. Post that, help identify the top terms from the toxic comments. \n",
    "\n",
    "Domain: Internet\n",
    "\n",
    "Analysis to be done: Build a text classification model using NLP and machine learning that detects toxic comments.\n",
    "\n",
    "Content: \n",
    "\n",
    "id: identifier number of the comment\n",
    "\n",
    "comment_text: the text in the comment\n",
    "\n",
    "toxic: 0 (non-toxic) /1 (toxic)\n",
    "\n",
    "Steps to perform:\n",
    "\n",
    "Cleanup the text data, using TF-IDF convert to vector space representation, use Support Vector Machines to detect toxic comments. Finally, get the list of top 15 toxic terms from the comments identified by the model.\n",
    "\n",
    "Tasks: \n",
    "\n",
    "Load the data using read_csv function from pandas package\n",
    "\n",
    "Get the comments into a list, for easy text cleanup and manipulation\n",
    "\n",
    "Cleanup: \n",
    "\n",
    "Using regular expressions, remove IP addresses\n",
    "\n",
    "Using regular expressions, remove URLs\n",
    "\n",
    "Normalize the casing\n",
    "\n",
    "Tokenize using word_tokenize from NLTK\n",
    "\n",
    "Remove stop words\n",
    "\n",
    "Remove punctuation\n",
    "\n",
    "Define a function to perform all these steps, you’ll use this later on the actual test set\n",
    "\n",
    "Using a counter, find the top terms in the data. \n",
    "\n",
    "Can any of these be considered contextual stop words? \n",
    "\n",
    "Words like “Wikipedia”, “page”, “edit” are examples of contextual stop words\n",
    "\n",
    "If yes, drop these from the data\n",
    "\n",
    "Separate into train and test sets\n",
    "\n",
    "Use train-test method to divide your data into 2 sets: train and test\n",
    "\n",
    "Use a 70-30 split\n",
    "\n",
    "Use TF-IDF values for the terms as feature to get into a vector space model\n",
    "\n",
    "Import TF-IDF vectorizer from sklearn\n",
    "\n",
    "Instantiate with a maximum of 4000 terms in your vocabulary\n",
    "\n",
    "Fit and apply on the train set\n",
    "\n",
    "Apply on the test set\n",
    "\n",
    "Model building: Support Vector Machine\n",
    "\n",
    "Instantiate SVC from sklearn with a linear kernel\n",
    "\n",
    "Fit on the train data\n",
    "\n",
    "Make predictions for the train and the test set\n",
    "\n",
    "Model evaluation: Accuracy, recall, and f1_score\n",
    "\n",
    "Report the accuracy on the train set\n",
    "\n",
    "Report the recall on the train set:decent, high, low?\n",
    "\n",
    "Get the f1_score on the train set\n",
    "\n",
    "Looks like you need to adjust  the class imbalance, as the model seems to focus on the 0s\n",
    "\n",
    "Adjust the appropriate parameter in the SVC module\n",
    "\n",
    "Train again with the adjustment and evaluate\n",
    "\n",
    "Train the model on the train set\n",
    "\n",
    "Evaluate the predictions on the validation set: accuracy, recall, f1_score\n",
    "\n",
    "Hyperparameter tuning\n",
    "\n",
    "Import GridSearch and StratifiedKFold (because of class imbalance)\n",
    "\n",
    "Provide the parameter grid to choose for ‘C’\n",
    "\n",
    "Use a balanced class weight while instantiating the Support Vector Classifier\n",
    "\n",
    "Find the parameters with the best recall in cross validation\n",
    "\n",
    "Choose ‘recall’ as the metric for scoring\n",
    "\n",
    "Choose stratified 5 fold cross validation scheme\n",
    "\n",
    "Fit on the train set\n",
    "\n",
    "What are the best parameters?\n",
    "\n",
    "Predict and evaluate using the best estimator\n",
    "\n",
    "Use best estimator from the grid search to make predictions on the test set\n",
    "\n",
    "What is the recall on the test set for the toxic comments?\n",
    "\n",
    "What is the f1_score?\n",
    "\n",
    "What are the most prominent terms in the toxic comments?\n",
    "\n",
    "Separate the comments from the test set that the model identified as toxic\n",
    "\n",
    "Make one large list of the terms\n",
    "\n",
    "Get the top 15 terms\n",
    "\n",
    "You can download the datasets from here  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4563\n",
       "1     437\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get the comments into a list, for easy text cleanup and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments=data['comment_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEAN UP\n",
    "Using regular expressions, remove URLs\n",
    "\n",
    "Normalize the casing\n",
    "\n",
    "Tokenize using word_tokenize from NLTK\n",
    "\n",
    "Remove stop words\n",
    "\n",
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "comments_noip=[re.sub('[\\d+\\.]','',txt) for txt in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"\\r\\n\\r\\n A barnstar for you! \\r\\n\\r\\n  The Defender of the Wiki Barnstar I like your edit on the Kayastha page Lets form a solidarity group against those who malign the article and its subject matter I propose the folloing name for the group\\r\\n\\r\\nUnited intellectuals\\' front of Kayastha ethinicty against racist or castist abuse (UIFKEARCA)   \"',\n",
       " '\"\\r\\n\\r\\nThis seems unbalanced  whatever I have said about Mathsci, he has said far more extreme and unpleasant things about me (not to mention others), and with much greater frequency  I\\'m more than happy to reign myself in, if that\\'s what you\\'d like (ruth be told, I was just trying to get Mathsci to pay attention and stop being uncivil)  I would expect you to issue the same request to Mathsci  \\r\\n\\r\\n If this is intentionally unbalanced (for whatever reason), please let me know, and I will voluntarily close this account and move on to other things  I like wikipedia, and I have a lot to contribute in my own way, but there is no point contributing to the project if some editors have administrative leave to be aggressively rude  I\\'m a good editor, and I don\\'t really deserve to have people riding my ass every time I try to do certain things  I\\'ll happily leave it in the hands of the drama-prone, if that\\'s what you think is best  Ludwigs \"',\n",
       " 'Marya Dzmitruk was born in Minsk, Belarus in March ,  Her mother, Olga Nikolaevna Moroz was born in Baranovichi, Belarus and her father was born in Brest, Belarus She is second child in the family Her parents divorced in  and soon after her father remarried and had two more children \\r\\nMarya, at the age of  began doing gymnastics, but quit two years later because she was denied a medal in a competition, where her age was incorrectly marked When she turned  years old, she got admitted to Music School # in Minsk, class of violin, and to Public School # with piano classes as a main course At the age of , Marya starred in Belarusfilm movie called “Dunechka” Soon after she started to play in theatre and was featured in television shows By  her mother decided to move to United States In September of  Marya went to her first American school, Ingrid B Lacy Middle School She graduated in Spring  and traveled back to Belarus for  months In August  she went to Oceana High School, from which she will graduate in  \\r\\n\\r\\nMarya Dzmitruk is a member of ISAR (International Society for Astrological Research), also a member of a non-profit government organization which deals with human rights abuse throughout the world (also known as Helsinki Committee) Marya holds two diplomas from music schools, four scholarships for Lisa Spector’s Music school and several awards from YLI (Youth Leadership Institute) \\r\\n\\r\\nMarya has a very close relationship with her mother Her personal life is “as happy as it could possibly get” a source says She is currently dating Alex K, from Odessa, Ukraine \\r\\n\\r\\nMarya currently attends school and works for ISAR and Helsinki Committee at her own free time',\n",
       " '\"\\r\\n\\r\\nTalkback\\r\\n\\r\\n Dear Celestia  \"',\n",
       " \"New Categories \\r\\n\\r\\nI honestly think that we need to add some more categories and move some characters into these new categories Like, characters such as Pete Ross or Whitney Fordman who haven't been on the show in a while, and are listed as main characters? That just doesn't make sense Perhaps creating a deceased/former characters section? I'll start doing it now If anyone has a problem, please post it in this discussion\",\n",
       " 'Looks like it is my spaces I guess I will start filtering for spaces as well Was trying not to do that cause it will bring in so many more hits But will do what we must ) -',\n",
       " '\"\\r\\n\\r\\nI would be willing to continue the plan if the plan is followed However, currently it is being used to harass and abuse me The way it is now a few editors can repeatedly harass me through the plan because they dislike or disagree with my opinion I am being held to a higher standards than other editors and punished if I err in situations where others are not punished or even warned for the same or worse behavior — (Talk) \"',\n",
       " \"loser - you can't block me forever you admin ego hippie freak\",\n",
       " 'YOU CANNOT BLOCK ME IF YOU BLOCK ME, I WILL CALL YOU A DUMBHEAD!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " 'I had not seen the source Gwern used I ran across only sites like this which would have been inappropriate to use as a source',\n",
       " '\"\\r\\n\\r\\n Hello Rick \\r\\n\\r\\nHope all is well I notice you\\'ve been editing occasionally, but the bot hasn\\'t been updating LOA (admin list) Is everything ok with the bot? \\'\\'\\'\\' message\\'\\' \"',\n",
       " 'of course, here you go [] [] [] [] What kind of references are you looking for?\\r\\n\\r\\nTHANK YOU!\\r\\nNr  is perfect Than we have both Gyulai and Csabai The problem is that I never discovered how to add files to the article',\n",
       " 'put a bigger delay in please \\r\\n\\r\\nEven if I try to zoom back seconds later to fix my failure, I get ECed  Make a longer delay please (bot is annoying enough)',\n",
       " 'YES/ Poland and Lithuania define marriage as a union between a man and a woman in thay constitutions just as several other countries in Europe eg Spain, France etc They are not mentioned here Why? \\r\\nAnd Latvia not only define marriage as a union between a man and a woman in its civil law and constitution (since ) but also bans same sex marriages in those acts So it is the only country in Europe to do so (first to BANN same sex marriage, and NOT third to define marriage)\\r\\nThis article sugests that Poland and Lithuania do the same and this is not truth! So, I say about it In my opinion, it should to be corrected',\n",
       " '\"\\r\\n\\r\\n Brilliant \\r\\nYou really are brilliant That\\'s  we\\'ve got on the software You must have the Bill Evans Conversations With Myself Ripper John - you\\'re better than we thought  —\\xa0Preceding unsigned comment added by  (talk • contribs)  \"',\n",
       " '\"\\r\\nNote: While undue weight is a legitimate concern, notability is not a limitation on the content of an article, only on the subject of article If something related to an individual is reported on in reliable secondary sources, you can be relatively certain it is of sufficient relevance to the individual to merit mention, article length permittingSkomorokh \"',\n",
       " 'Theres a fucking wiki page on it you insane person\\r\\n\\r\\nhttp://enwikipediaorg/wiki/Mutilation',\n",
       " '\"New edits - explanations and justifications ==\\r\\n\\r\\nConsidering the controversy about the definition of Druz as Arabs, we\\'d better of say \"\"Arab and Arabic speaking people that are not Jewish\"\" The Druz may or may not be Arabs, but they definitely speak Arabic\\r\\nSome of the , Palestinian Arabs who became Israeli citizens in  were refugees who managed to return to their homes before the borders closed By the way, I wonder if we should write \"\"Palestinian and Bedouin Arabs\"\", as the Bedouins often distinguish themselves from the Palestinians\\r\\n\"\"Occupied East Jerusalem\"\" - let us do without the complicated discussion about the legal status of Jerusalem Saying that East Jerusalem was unilaterlly annexed is quite enough in terms of supplying required information\\r\\nIt is improtant to emphasize the special status of the Druze\\r\\nIt is important to emphasize the special status of Jerusalemite Arabs\\r\\nThe regard of Israeli Arabs as an indiginous minority is a theory or a claim It is not a fact It is very hard to determine which of the Arabs in Mandatory Palestine is an immigrant from other parts of the Middle East, or a descendant of such an immigrant, and which of them is actually indigenous This way or another, the conflict between Israel and its Arab citizens cannot be said to be the mere result of a universal phenomenon of tension between indigenous people and newly established authorities The main cause of tension is the Israeli Arab conflict  \\r\\n\\r\\n== \"',\n",
       " 'Your help is not needed here  - thank you',\n",
       " \"Apologies\\r\\n\\r\\nHello, TJive I'm really sorry for all the trouble I caused you on WP I promise will no longer automatically revert your edits, but discuss them You're welcome to keep watch of my contributions Please forgive me\",\n",
       " '\"\\r\\nAh, you\\'re busy Have a pleasant trip?  (talk) \"',\n",
       " 'Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!',\n",
       " '\"\\r\\n\\r\\n Unfair Accusation of Spamming? \\r\\n\\r\\nMoved from my talk page\\r\\nHello\\r\\n\\r\\nI feel that my addition of external links to the Wikipedia pages \"\"Genesis creation narrative,\"\" \"\"Ibbur,\"\" and \"\"Gilgul\"\" have been unfairly flagged for inappropriate activity The external links added are primary source texts directly related to the Wikipedia page subject  They are not commercial or ideological Please advise\\r\\n\\r\\nThank you \\r\\nJeangohile\\r\\n\\r\\nI removed them because they linked to a blog and blogs are not considered reliable sources for Wikipedia, if you feel they are a special case you could discuss on the article talk pages This link may be helpful links normally to be avoided All good wishes   \\r\\n\\r\\nOk I understand and thanks for responding I still feel my external links provide important primary source texts that are not available online Yes they are presented on a blog but there is no writing of my own presented Please reconsider\\r\\nBest regards  — Preceding unsigned comment added by  (talk • contribs)  \"',\n",
       " 'So, are you a Christian becaue of Jesus or because Assyrian women and their tits? Do Assyrian women have the best titt-fucking tits in the Middle East? — |Talk :  Aug,  (UTC)',\n",
       " 'Protected? \\r\\n\\r\\nWhy is this nation in particular protected from editing? It is not that I want to edit it or let any one vandalize but that is selectivism Be in mind that the united states is not the only nation in the world Some one has got to put a sign that says uneditable if it is not editable',\n",
       " '\"\\r\\nI\\'d be honored I am pleased that my reputation proceeds me I must warn you, though, I am pretty thorough - just ask Javert about all the nitpicking I\\'ve done to his articles!  (T · C) \"',\n",
       " \"Thanks \\r\\n\\r\\n  I'll read up on the requirements\\r\\n\\r\\nVince\",\n",
       " 'Your are a filthy fuck \\r\\n\\r\\ngo fuck your self',\n",
       " '\" (Politifact) and this quote: \"\"Their identities are an open secret\"\" (WSJ)\"',\n",
       " 'I APPROVE THAT PREVIOUS TEXT TO BE ON MY TALK PAGE, SIR! I HAVE A RIGHT TO HAVE  WHATEVER I WANT ON IT!']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_noip[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "comments_lower=[txt.lower() for txt in comments_noip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"\\r\\n\\r\\n a barnstar for you! \\r\\n\\r\\n  the defender of the wiki barnstar i like your edit on the kayastha page lets form a solidarity group against those who malign the article and its subject matter i propose the folloing name for the group\\r\\n\\r\\nunited intellectuals\\' front of kayastha ethinicty against racist or castist abuse (uifkearca)   \"',\n",
       " '\"\\r\\n\\r\\nthis seems unbalanced  whatever i have said about mathsci, he has said far more extreme and unpleasant things about me (not to mention others), and with much greater frequency  i\\'m more than happy to reign myself in, if that\\'s what you\\'d like (ruth be told, i was just trying to get mathsci to pay attention and stop being uncivil)  i would expect you to issue the same request to mathsci  \\r\\n\\r\\n if this is intentionally unbalanced (for whatever reason), please let me know, and i will voluntarily close this account and move on to other things  i like wikipedia, and i have a lot to contribute in my own way, but there is no point contributing to the project if some editors have administrative leave to be aggressively rude  i\\'m a good editor, and i don\\'t really deserve to have people riding my ass every time i try to do certain things  i\\'ll happily leave it in the hands of the drama-prone, if that\\'s what you think is best  ludwigs \"',\n",
       " 'marya dzmitruk was born in minsk, belarus in march ,  her mother, olga nikolaevna moroz was born in baranovichi, belarus and her father was born in brest, belarus she is second child in the family her parents divorced in  and soon after her father remarried and had two more children \\r\\nmarya, at the age of  began doing gymnastics, but quit two years later because she was denied a medal in a competition, where her age was incorrectly marked when she turned  years old, she got admitted to music school # in minsk, class of violin, and to public school # with piano classes as a main course at the age of , marya starred in belarusfilm movie called “dunechka” soon after she started to play in theatre and was featured in television shows by  her mother decided to move to united states in september of  marya went to her first american school, ingrid b lacy middle school she graduated in spring  and traveled back to belarus for  months in august  she went to oceana high school, from which she will graduate in  \\r\\n\\r\\nmarya dzmitruk is a member of isar (international society for astrological research), also a member of a non-profit government organization which deals with human rights abuse throughout the world (also known as helsinki committee) marya holds two diplomas from music schools, four scholarships for lisa spector’s music school and several awards from yli (youth leadership institute) \\r\\n\\r\\nmarya has a very close relationship with her mother her personal life is “as happy as it could possibly get” a source says she is currently dating alex k, from odessa, ukraine \\r\\n\\r\\nmarya currently attends school and works for isar and helsinki committee at her own free time',\n",
       " '\"\\r\\n\\r\\ntalkback\\r\\n\\r\\n dear celestia  \"',\n",
       " \"new categories \\r\\n\\r\\ni honestly think that we need to add some more categories and move some characters into these new categories like, characters such as pete ross or whitney fordman who haven't been on the show in a while, and are listed as main characters? that just doesn't make sense perhaps creating a deceased/former characters section? i'll start doing it now if anyone has a problem, please post it in this discussion\",\n",
       " 'looks like it is my spaces i guess i will start filtering for spaces as well was trying not to do that cause it will bring in so many more hits but will do what we must ) -',\n",
       " '\"\\r\\n\\r\\ni would be willing to continue the plan if the plan is followed however, currently it is being used to harass and abuse me the way it is now a few editors can repeatedly harass me through the plan because they dislike or disagree with my opinion i am being held to a higher standards than other editors and punished if i err in situations where others are not punished or even warned for the same or worse behavior — (talk) \"',\n",
       " \"loser - you can't block me forever you admin ego hippie freak\",\n",
       " 'you cannot block me if you block me, i will call you a dumbhead!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " 'i had not seen the source gwern used i ran across only sites like this which would have been inappropriate to use as a source',\n",
       " '\"\\r\\n\\r\\n hello rick \\r\\n\\r\\nhope all is well i notice you\\'ve been editing occasionally, but the bot hasn\\'t been updating loa (admin list) is everything ok with the bot? \\'\\'\\'\\' message\\'\\' \"',\n",
       " 'of course, here you go [] [] [] [] what kind of references are you looking for?\\r\\n\\r\\nthank you!\\r\\nnr  is perfect than we have both gyulai and csabai the problem is that i never discovered how to add files to the article',\n",
       " 'put a bigger delay in please \\r\\n\\r\\neven if i try to zoom back seconds later to fix my failure, i get eced  make a longer delay please (bot is annoying enough)',\n",
       " 'yes/ poland and lithuania define marriage as a union between a man and a woman in thay constitutions just as several other countries in europe eg spain, france etc they are not mentioned here why? \\r\\nand latvia not only define marriage as a union between a man and a woman in its civil law and constitution (since ) but also bans same sex marriages in those acts so it is the only country in europe to do so (first to bann same sex marriage, and not third to define marriage)\\r\\nthis article sugests that poland and lithuania do the same and this is not truth! so, i say about it in my opinion, it should to be corrected',\n",
       " '\"\\r\\n\\r\\n brilliant \\r\\nyou really are brilliant that\\'s  we\\'ve got on the software you must have the bill evans conversations with myself ripper john - you\\'re better than we thought  —\\xa0preceding unsigned comment added by  (talk • contribs)  \"',\n",
       " '\"\\r\\nnote: while undue weight is a legitimate concern, notability is not a limitation on the content of an article, only on the subject of article if something related to an individual is reported on in reliable secondary sources, you can be relatively certain it is of sufficient relevance to the individual to merit mention, article length permittingskomorokh \"',\n",
       " 'theres a fucking wiki page on it you insane person\\r\\n\\r\\nhttp://enwikipediaorg/wiki/mutilation',\n",
       " '\"new edits - explanations and justifications ==\\r\\n\\r\\nconsidering the controversy about the definition of druz as arabs, we\\'d better of say \"\"arab and arabic speaking people that are not jewish\"\" the druz may or may not be arabs, but they definitely speak arabic\\r\\nsome of the , palestinian arabs who became israeli citizens in  were refugees who managed to return to their homes before the borders closed by the way, i wonder if we should write \"\"palestinian and bedouin arabs\"\", as the bedouins often distinguish themselves from the palestinians\\r\\n\"\"occupied east jerusalem\"\" - let us do without the complicated discussion about the legal status of jerusalem saying that east jerusalem was unilaterlly annexed is quite enough in terms of supplying required information\\r\\nit is improtant to emphasize the special status of the druze\\r\\nit is important to emphasize the special status of jerusalemite arabs\\r\\nthe regard of israeli arabs as an indiginous minority is a theory or a claim it is not a fact it is very hard to determine which of the arabs in mandatory palestine is an immigrant from other parts of the middle east, or a descendant of such an immigrant, and which of them is actually indigenous this way or another, the conflict between israel and its arab citizens cannot be said to be the mere result of a universal phenomenon of tension between indigenous people and newly established authorities the main cause of tension is the israeli arab conflict  \\r\\n\\r\\n== \"',\n",
       " 'your help is not needed here  - thank you',\n",
       " \"apologies\\r\\n\\r\\nhello, tjive i'm really sorry for all the trouble i caused you on wp i promise will no longer automatically revert your edits, but discuss them you're welcome to keep watch of my contributions please forgive me\"]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_lower[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URL\n",
    "comments_nourl=[re.sub('\\w+://\\s+','',txt) for txt in comments_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove '\n",
    "comments_nourl=[txt.replace(\"\\'\",\"\") for txt in comments_nourl]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_tokens=[word_tokenize(sent) for sent in comments_nourl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'a', 'barnstar', 'for', 'you', '!', 'the', 'defender', 'of', 'the', 'wiki', 'barnstar', 'i', 'like', 'your', 'edit', 'on', 'the', 'kayastha', 'page', 'lets', 'form', 'a', 'solidarity', 'group', 'against', 'those', 'who', 'malign', 'the', 'article', 'and', 'its', 'subject', 'matter', 'i', 'propose', 'the', 'folloing', 'name', 'for', 'the', 'group', 'united', 'intellectuals', 'front', 'of', 'kayastha', 'ethinicty', 'against', 'racist', 'or', 'castist', 'abuse', '(', 'uifkearca', ')', '``']\n"
     ]
    }
   ],
   "source": [
    "print(comments_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nltk=stopwords.words('english')\n",
    "stop_punc=list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_final=stop_nltk+stop_punc+[\"must\", \"would\", 'could', \"'s\", \"n't\", \"'m\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"''\", '``','...','•','—',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stop(sent):\n",
    "    return [term for term in sent if term not in stop_final and stop_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barnstar', 'defender', 'wiki', 'barnstar', 'like', 'edit', 'kayastha', 'page', 'lets', 'form', 'solidarity', 'group', 'malign', 'article', 'subject', 'matter', 'propose', 'folloing', 'name', 'group', 'united', 'intellectuals', 'front', 'kayastha', 'ethinicty', 'racist', 'castist', 'abuse', 'uifkearca']\n"
     ]
    }
   ],
   "source": [
    "print(del_stop(comments_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean=[del_stop(sent) for sent in comments_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a counter, find the top terms in the data.\n",
    "\n",
    "Can any of these be considered contextual stop words?\n",
    "\n",
    "Words like “Wikipedia”, “page”, “edit” are examples of contextual stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list=[]\n",
    "for sent in comments_clean:\n",
    "    term_list.extend(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('article', 1662),\n",
       " ('page', 1510),\n",
       " ('wikipedia', 1344),\n",
       " ('talk', 1172),\n",
       " ('please', 1039),\n",
       " ('ass', 986),\n",
       " ('fuck', 907),\n",
       " ('one', 858),\n",
       " ('like', 837),\n",
       " ('dont', 781),\n",
       " ('also', 658),\n",
       " ('think', 630),\n",
       " ('see', 630),\n",
       " ('know', 595),\n",
       " ('im', 561),\n",
       " ('edit', 560),\n",
       " ('articles', 549),\n",
       " ('use', 548),\n",
       " ('people', 538),\n",
       " ('name', 536)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=Counter(term_list)\n",
    "res.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_context= ['article', 'page', 'pages', 'wikipedia', 'wiki', 'talk', 'please', 'also', 'may', 'edit', 'edits', \n",
    "                         'articles', 'user', 'information', 'sources', 'source', 'content', 'wp', 'discussion', 'subject', \n",
    "                         'editor', 'editors', 'copyright', 'contributions'] \n",
    "comments_clean=[del_stop(sent) for sent in comments_tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_clean=[\" \".join(sent) for sent in comments_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['talkback dear celestia',\n",
       " 'new categories honestly think need add categories move characters new categories like characters pete ross whitney fordman havent show listed main characters doesnt make sense perhaps creating deceased/former characters section ill start anyone problem please post discussion']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_clean[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Separate into train and test sets\n",
    "\n",
    "Use train-test method to divide your data into 2 sets: train and test\n",
    "Use a 70-30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=comments_clean\n",
    "y=data.toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use TF-IDF values for the terms as feature to get into a vector space model\n",
    "\n",
    "Import TF-IDF vectorizer from sklearn\n",
    "Instantiate with a maximum of 4000 terms in your vocabulary\n",
    "Fit and apply on the train set\n",
    "Apply on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(max_features=4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf=vectorizer.fit_transform(X_train)\n",
    "X_test_tf=vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 4000)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 4000)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "from sklearn.svm import SVC\n",
    "classifier_svc=SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classifier_svc.fit(X_train_tf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This SVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-5f0c897db08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \"\"\"\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_ties\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function_shape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ovo\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This SVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "y_pred_test=classifier_svc.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=classifier_svc.predict(X_train_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train,y_pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ooks like you need to adjust the class imbalance, as the model seems to focus on the 0s\n",
    "\n",
    "Adjust the appropriate parameter in the SVC module\n",
    "\n",
    "Train again with the adjustment and evaluate\n",
    "\n",
    "Train the model on the train set\n",
    "\n",
    "Evaluate the predictions on the validation set: accuracy, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_svm_bal=SVC(kernel='linear',class_weight='balanced')\n",
    "\n",
    "classifier_svm_bal.fit(X_train_tf,y_train)\n",
    "y_pred_test=classifier_svm_bal.predict(X_test_tf)\n",
    "\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=classifier_svm_bal.predict(X_train_tf)\n",
    "print(classification_report(y_train,y_pred_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'C':[0.1,1,10,1000,10000,10000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_grid=SVC(kernel='linear',class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(estimator=classifier_grid,param_grid=param_grid,\n",
    "                        cv=StratifiedKFold(5),scoring='recall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_tf,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=grid_search.best_estimator_.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments=pd.Series(X_test)[y_test_pred==1].values\n",
    "len(toxic_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list=[]\n",
    "for comment in toxic_comments:\n",
    "    term_list.extend(word_tokenize(comment))\n",
    "cts=Counter(term_list)\n",
    "cts.most_common(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "title = 'Word Cloud Analysis for Text Data'\n",
    "wordcloud = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=None,\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=3,\n",
    "    random_state=1 \n",
    ").generate(\" \".join(term_list))\n",
    "\n",
    "fig = plt.figure(1, figsize=(10, 10))\n",
    "plt.axis('off')\n",
    "if title: \n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.subplots_adjust(top=2.3)\n",
    "    \n",
    "plt.imshow(wordcloud);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
